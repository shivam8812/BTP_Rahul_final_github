{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGEaA6ggpoza"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import urllib.request\n",
        "import matplotlib.dates as mdates\n",
        "import datetime as Date\n",
        "from geopy.geocoders import Nominatim\n",
        "pd.set_option('display.max_columns',None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "\n",
        "url = \"https://power.larc.nasa.gov/api/temporal/daily/point?parameters=T2M_MAX,T2M_MIN,RH2M,PRECTOTCORR,PS,WS50M_MAX,WS50M_MIN&community=RE&longitude=71.4000&latitude=19.2000&start=20050101&end=20221231&format=CSV\"\n",
        "# Assuming the dataset is in a CSV file with columns: date, param1, param2, ..., param7\n",
        "\n",
        "urllib.request.urlretrieve(url,'climate.csv')\n",
        "df = pd.read_csv('climate.csv',skiprows = 15)\n",
        "\n",
        "df['YEAR'] = df.YEAR.astype(str)\n",
        "df['MO'] = df.MO.astype(str)\n",
        "df['DY'] = df.DY.astype(str)\n",
        "\n",
        "df['date'] = df['YEAR'].str.cat(df['MO'],sep = '/')\n",
        "df['DATE'] = df['date'].str.cat(df['DY'],sep = '/')\n",
        "df.drop(columns = ['YEAR','MO','DY','date','DATE'],axis=1,inplace=True)\n",
        "\n",
        "# Normalize the data using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df.values)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "data = torch.from_numpy(df_scaled).float()"
      ],
      "metadata": {
        "id": "JkzqG4d3s2k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "WpKNDVJ5tJH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ffc51b-7ab4-43a9-e5d1-05daa30dc47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6574, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn1 = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.rnn2 = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.rnn3 = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.rnn4 = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden1 = self.init_hidden(batch_size)\n",
        "        hidden2 = self.init_hidden(batch_size)\n",
        "        hidden3 = self.init_hidden(batch_size)\n",
        "        hidden4 = self.init_hidden(batch_size)\n",
        "\n",
        "        out, hidden1 = self.rnn1(x, hidden1)\n",
        "        # out = self.dropout1(self.bn1(out))\n",
        "        out, hidden2 = self.rnn2(out, hidden2)\n",
        "        # out = self.dropout2(self.bn2(out))\n",
        "        out, hidden3 = self.rnn3(out, hidden3)\n",
        "        # out = self.dropout3(self.bn3(out))\n",
        "        out, hidden4 = self.rnn4(out, hidden4)\n",
        "        # out = self.dropout4(self.bn4(out))\n",
        "        \n",
        "        out = self.fc(out[:, -1:, :]) # select the output for the last 1 days\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "K7RwaQ-usiUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize hyperparameters\n",
        "input_size = 7  # 7 weather parameters\n",
        "hidden_size = 64   # specify your desired hidden size\n",
        "num_layers = 2     # specify your desired number of layers\n",
        "output_size = 7   # 7 weather parameters x 7 days\n",
        "seq_length = 15  # Number of previous days to consider as input\n",
        "target_length = 1 # Number of future days to consider as output\n",
        "batch_size = 32  # Batch size for training\n",
        "learning_rate = 0.001  # Learning rate for optimizer\n",
        "num_epochs = 50  # Number of epochs for training\n",
        "\n"
      ],
      "metadata": {
        "id": "aFFci7S0tARY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and targets for predicting the next 7 days\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data) - seq_length - target_length):\n",
        "    seq = data[i:i + seq_length, :]\n",
        "    target = data[i + seq_length:i + seq_length + target_length, :]\n",
        "    X.append(seq)\n",
        "    y.append(target)\n",
        "X = torch.stack(X)\n",
        "y = torch.stack(y)\n",
        "\n",
        "# Calculate sizes for train, validation, and test sets\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size = int(0.15 * len(X))\n",
        "test_size = len(X) - train_size - val_size\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_X, val_X, test_X = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
        "train_y, val_y, test_y = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(val_X.shape)\n",
        "print(val_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "val_dataset = torch.utils.data.TensorDataset(val_X, val_y)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create DataLoader for test set\n",
        "test_dataset = torch.utils.data.TensorDataset(test_X, test_y)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Ggo9sl0UJAS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4215c0-e2fe-45ba-9de0-f2d7bc5755be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4590, 15, 7])\n",
            "torch.Size([4590, 1, 7])\n",
            "torch.Size([983, 15, 7])\n",
            "torch.Size([983, 1, 7])\n",
            "torch.Size([985, 15, 7])\n",
            "torch.Size([985, 1, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "# Create an instance of the hybrid model\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean squared error loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "train_losses = []  # List to store training losses for each epoch\n",
        "val_losses = []  # List to store validation losses for each epoch\n",
        "best_loss = float('inf')  # Set initial best loss to positive infinity\n",
        "best_model_state_dict = None  # Variable to store the state_dict of the best model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_train_loss = 0.0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()\n",
        "    epoch_train_loss /= len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            epoch_val_loss += criterion(outputs, targets).item()\n",
        "    epoch_val_loss /= len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    # Update the best model if the current model has a lower validation loss\n",
        "    if epoch_val_loss < best_loss:\n",
        "        best_loss = epoch_val_loss\n",
        "        best_model_state_dict = model.state_dict()\n",
        "        torch.save(best_model_state_dict, 'best_model.pth')  # Save the best model to a file\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "# Plot the training and validation loss graphs\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'Training complete. Best model saved with validation loss: {best_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "CcyzNgdVtEn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "# Load the saved state_dict of the best model\n",
        "best_model_state_dict = torch.load('best_model.pth')\n",
        "\n",
        "# Set the model's state_dict to the loaded state_dict\n",
        "model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_X)\n",
        "    test_loss = criterion(test_outputs, test_y)\n",
        "print('Test Loss: {:.4f}'.format(test_loss.item()))"
      ],
      "metadata": {
        "id": "slEOa7MNuIid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted = model(test_X)\n",
        "\n",
        "# # # Denormalize the predicted values\n",
        "# denormalized_tensor = scaler.inverse_transform(predicted.view(-1, 7)).view(580, 7, 7)\n",
        "# denormalized_tensor_y = scaler.inverse_transform(test_y.view(-1, 7)).view(580, 7, 7)\n",
        "# predicted.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "qSOTAQVXzyFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denormalized_tensor = torch.empty_like(predicted)\n",
        "denormalized_tensor_y = torch.empty_like(test_y)\n",
        "for i in range(predicted.shape[0]):\n",
        "    for j in  range(0,1):\n",
        "      num = predicted[i][j].numpy()\n",
        "      nump = np.array(num)\n",
        "      numpp = nump.reshape(1,7)\n",
        "      denorm = scaler.inverse_transform(numpp)\n",
        "      denormm = torch.from_numpy(denorm)\n",
        "      denormalized_tensor[i][j] = denormm\n",
        "for i in range(test_y.shape[0]):\n",
        "    for j in  range(0,1):\n",
        "      num = test_y[i][j].numpy()\n",
        "      nump = np.array(num)\n",
        "      numpp = nump.reshape(1,7)\n",
        "      denorm = scaler.inverse_transform(numpp)\n",
        "      denormm = torch.from_numpy(denorm)\n",
        "      denormalized_tensor_y[i][j] = denormm\n",
        "for i in range(0,10):\n",
        "  print(denormalized_tensor[i])\n",
        "  print(\"/////\")\n",
        "  print(denormalized_tensor_y[i])\n",
        "  print(\"//////////////////////////////////////////////////////////////////////////\")"
      ],
      "metadata": {
        "id": "2s3Z6nbW5WoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(train_X[i])\n",
        "  print(\"/////\")\n",
        "  print(train_y[i])\n",
        "  print(\"//////////////////////////////////////////////////////////////////////////\")"
      ],
      "metadata": {
        "id": "RTUHNB5D7Yz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "from torchsummary import summary\n",
        "input_e = (15, 7)\n",
        "input_tensor = torch.randn(input_size)\n",
        "summary(model, input_size=input_e)"
      ],
      "metadata": {
        "id": "_IjHgfWyA-R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "# Set your model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the device to use (e.g., \"cuda\" for GPU, \"cpu\" for CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Initialize variables for total inference time and total number of batches\n",
        "total_time = 0.0\n",
        "total_batches = 0\n",
        "\n",
        "# Iterate through the test loader\n",
        "for batch_idx, (input, target) in enumerate(test_loader):\n",
        "    # Move the batch of test data to the appropriate device\n",
        "    input = input.to(device)\n",
        "\n",
        "    # Measure the start time of inference\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Disable gradient computation during inference\n",
        "    with torch.no_grad():\n",
        "        # Pass the batch of test data through the model\n",
        "        output = model(input)\n",
        "\n",
        "    # Measure the end time of inference\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Compute the inference time for the batch\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Accumulate the inference time and update the total number of batches\n",
        "    total_time += inference_time\n",
        "    total_batches += 1\n",
        "\n",
        "# Calculate the average inference time\n",
        "avg_inference_time = total_time / total_batches\n",
        "\n",
        "# Print the average inference time in seconds\n",
        "print(\"Average Inference Time: {:.4f} seconds\".format(avg_inference_time))\n"
      ],
      "metadata": {
        "id": "y9jke2Q9BLq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.fc.parameters() if p.requires_grad)\n",
        "print(\"Total trainable parameters in GRU layer:\", total_params)"
      ],
      "metadata": {
        "id": "gfbn5tX0Bwst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_GeCmTKJQQXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}