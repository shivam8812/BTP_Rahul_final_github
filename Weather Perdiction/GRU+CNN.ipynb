{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGEaA6ggpoza"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import urllib.request\n",
        "import matplotlib.dates as mdates\n",
        "import datetime as Date\n",
        "from geopy.geocoders import Nominatim\n",
        "pd.set_option('display.max_columns',None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "\n",
        "url = \"https://power.larc.nasa.gov/api/temporal/daily/point?parameters=T2M_MAX,T2M_MIN,RH2M,PRECTOTCORR,PS,WS50M_MAX,WS50M_MIN&community=RE&longitude=71.4000&latitude=19.2000&start=20050101&end=20221231&format=CSV\"\n",
        "# Assuming the dataset is in a CSV file with columns: date, param1, param2, ..., param7\n",
        "\n",
        "urllib.request.urlretrieve(url,'climate.csv')\n",
        "df = pd.read_csv('climate.csv',skiprows = 15)\n",
        "\n",
        "df['YEAR'] = df.YEAR.astype(str)\n",
        "df['MO'] = df.MO.astype(str)\n",
        "df['DY'] = df.DY.astype(str)\n",
        "\n",
        "df['date'] = df['YEAR'].str.cat(df['MO'],sep = '/')\n",
        "df['DATE'] = df['date'].str.cat(df['DY'],sep = '/')\n",
        "df.drop(columns = ['YEAR','MO','DY','date','DATE'],axis=1,inplace=True)\n",
        "\n",
        "# Normalize the data using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df.values)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "data = torch.from_numpy(df_scaled).float()"
      ],
      "metadata": {
        "id": "JkzqG4d3s2k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpKNDVJ5tJH_",
        "outputId": "058030cc-8135-4b1a-a8d8-b9af4889d9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6574, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, cnn_channels, kernel_size, seq_len, dropout_prob=0.5):\n",
        "        super(WeatherModel, self).__init__()\n",
        "        self.gru1 = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.conv1 = nn.Conv2d(1, cnn_channels, kernel_size)\n",
        "        self.conv2 = nn.Conv2d(cnn_channels, cnn_channels*2, kernel_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(hidden_dim * seq_len + cnn_channels*2 * (hidden_dim - kernel_size*2 + 2) * (seq_len- kernel_size*2 + 2), hidden_dim*2)\n",
        "        self.fc2 = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(cnn_channels)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(cnn_channels*2)\n",
        "        self.batch_norm_fc1 = nn.BatchNorm1d(hidden_dim*2)\n",
        "        self.batch_norm_fc2 = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out1, _ = self.gru1(x)\n",
        "        gru_out2, _ = self.gru2(gru_out1)\n",
        "        gru_out2 = gru_out2.unsqueeze(1).permute(0, 1, 3, 2)  \n",
        "\n",
        "        conv1_out = self.conv1(gru_out2)\n",
        "        conv1_out = self.batch_norm1(conv1_out)\n",
        "        conv1_out = self.relu(conv1_out)\n",
        "\n",
        "        conv2_out = self.conv2(conv1_out)\n",
        "        conv2_out = self.batch_norm2(conv2_out)\n",
        "        conv2_out = self.relu(conv2_out)\n",
        "\n",
        "        conv2_out = torch.flatten(conv2_out, start_dim=1)\n",
        "        gru_out2 = gru_out2.squeeze(1)\n",
        "        gru_out2 = torch.flatten(gru_out2, start_dim=1)\n",
        "\n",
        "        combined_out = torch.cat((gru_out2, conv2_out), dim=1)\n",
        "\n",
        "        fc1_out = self.fc1(combined_out)\n",
        "        fc1_out = self.batch_norm_fc1(fc1_out)\n",
        "        fc1_out = self.relu(fc1_out)\n",
        "        # fc1_out = self.dropout(fc1_out)\n",
        "\n",
        "        output = self.fc2(fc1_out)\n",
        "        fc2_out = self.relu(output)\n",
        "        # output = self.batch_norm_fc2(output)\n",
        "\n",
        "        return fc2_out\n"
      ],
      "metadata": {
        "id": "K7RwaQ-usiUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize hyperparameters\n",
        "\n",
        "seq_length = 15  # Number of previous days to consider as input\n",
        "target_length = 1 # Number of future days to consider as output\n",
        "batch_size = 32  # Batch size for training\n",
        "learning_rate = 0.001  # Learning rate for optimizer\n",
        "num_epochs = 50  # Number of epochs for training\n",
        "input_dim = 7  # Number of weather parameters\n",
        "hidden_dim = 64  # Hidden dimension of GRU\n",
        "output_dim = 7  # Output dimension, same as input_dim for predicting weather parameters\n",
        "cnn_channels = 32  # Number of channels in CNN\n",
        "kernel_size = 3  # Kernel size in CNN\n"
      ],
      "metadata": {
        "id": "aFFci7S0tARY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and targets for predicting the next 7 days\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data) - seq_length - target_length):\n",
        "    seq = data[i:i + seq_length, :]\n",
        "    target = data[i + seq_length:i + seq_length + target_length, :]\n",
        "    X.append(seq)\n",
        "    y.append(target)\n",
        "X = torch.stack(X)\n",
        "y = torch.stack(y)\n",
        "\n",
        "# Calculate sizes for train, validation, and test sets\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size = int(0.15 * len(X))\n",
        "test_size = len(X) - train_size - val_size\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_X, val_X, test_X = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
        "train_y, val_y, test_y = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(val_X.shape)\n",
        "print(val_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "val_dataset = torch.utils.data.TensorDataset(val_X, val_y)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create DataLoader for test set\n",
        "test_dataset = torch.utils.data.TensorDataset(test_X, test_y)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Ggo9sl0UJAS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe270c5-e539-4f4f-fd07-f580ae96353b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4590, 15, 7])\n",
            "torch.Size([4590, 1, 7])\n",
            "torch.Size([983, 15, 7])\n",
            "torch.Size([983, 1, 7])\n",
            "torch.Size([985, 15, 7])\n",
            "torch.Size([985, 1, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "# Create an instance of the hybrid model\n",
        "model = WeatherModel(input_dim, hidden_dim, output_dim, cnn_channels, kernel_size, seq_length)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean squared error loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "train_losses = []  # List to store training losses for each epoch\n",
        "val_losses = []  # List to store validation losses for each epoch\n",
        "best_loss = float('inf')  # Set initial best loss to positive infinity\n",
        "best_model_state_dict = None  # Variable to store the state_dict of the best model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_train_loss = 0.0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()\n",
        "    epoch_train_loss /= len(train_loader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            epoch_val_loss += criterion(outputs, targets).item()\n",
        "    epoch_val_loss /= len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    # Update the best model if the current model has a lower validation loss\n",
        "    if epoch_val_loss < best_loss:\n",
        "        best_loss = epoch_val_loss\n",
        "        best_model_state_dict = model.state_dict()\n",
        "        torch.save(best_model_state_dict, 'best_model.pth')  # Save the best model to a file\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "# Plot the training and validation loss graphs\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'Training complete. Best model saved with validation loss: {best_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "CcyzNgdVtEn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "# Load the saved state_dict of the best model\n",
        "best_model_state_dict = torch.load('best_model.pth')\n",
        "\n",
        "# Set the model's state_dict to the loaded state_dict\n",
        "model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_X)\n",
        "    test_loss = criterion(test_outputs, test_y)\n",
        "print('Test Loss: {:.4f}'.format(test_loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slEOa7MNuIid",
        "outputId": "e34c3dee-1a6c-4696-cd04-d87281cca60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([985, 1, 7])) that is different to the input size (torch.Size([985, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted = model(test_X)\n",
        "\n",
        "# # # Denormalize the predicted values\n",
        "# denormalized_tensor = scaler.inverse_transform(predicted.view(-1, 7)).view(580, 7, 7)\n",
        "# denormalized_tensor_y = scaler.inverse_transform(test_y.view(-1, 7)).view(580, 7, 7)\n",
        "predicted.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "qSOTAQVXzyFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86aabe6e-a4e4-4656-b908-3c4213d2713e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([985, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the scaling parameters\n",
        "print(\"Minimum values for each feature: \", scaler.data_min_)\n",
        "print(\"Maximum values for each feature: \", scaler.data_max_)\n",
        "print(\"Range of values for each feature: \", scaler.data_range_)\n",
        "print(\"Scaling factors for each feature: \", scaler.scale_)\n",
        "print(\"Minimum values used for scaling: \", scaler.min_)\n",
        "print(\"Scaling factors used for scaling: \", scaler.scale_)"
      ],
      "metadata": {
        "id": "HRYwk3RLz7A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denormalized_tensor = torch.empty_like(predicted)\n",
        "denormalized_tensor_y = torch.empty_like(test_y)\n",
        "for i in range(predicted.shape[0]):\n",
        "    for j in  range(0,1):\n",
        "      num = predicted[i].numpy()\n",
        "      nump = np.array(num)\n",
        "      numpp = nump.reshape(1,7)\n",
        "      denorm = scaler.inverse_transform(numpp)\n",
        "      denormm = torch.from_numpy(denorm)\n",
        "      denormalized_tensor[i] = denormm\n",
        "for i in range(test_y.shape[0]):\n",
        "    for j in  range(0,1):\n",
        "      num = test_y[i][j].numpy()\n",
        "      nump = np.array(num)\n",
        "      numpp = nump.reshape(1,7)\n",
        "      denorm = scaler.inverse_transform(numpp)\n",
        "      denormm = torch.from_numpy(denorm)\n",
        "      denormalized_tensor_y[i][j] = denormm\n",
        "for i in range(0,10):\n",
        "  print(denormalized_tensor[i])\n",
        "  print(\"/////\")\n",
        "  print(denormalized_tensor_y[i])\n",
        "  print(\"//////////////////////////////////////////////////////////////////////////\")"
      ],
      "metadata": {
        "id": "2s3Z6nbW5WoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "input_size = [15, 7]   # Input size of your model\n",
        "summary(model, input_size=input_size)"
      ],
      "metadata": {
        "id": "5zNFCXOz7qfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainable parameters in GRU layer:\", total_params)"
      ],
      "metadata": {
        "id": "B1-mpgNarHhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "# Set your model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the device to use (e.g., \"cuda\" for GPU, \"cpu\" for CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Initialize variables for total inference time and total number of batches\n",
        "total_time = 0.0\n",
        "total_batches = 0\n",
        "\n",
        "# Iterate through the test loader\n",
        "for batch_idx, (input, target) in enumerate(test_loader):\n",
        "    # Move the batch of test data to the appropriate device\n",
        "    input = input.to(device)\n",
        "\n",
        "    # Measure the start time of inference\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Disable gradient computation during inference\n",
        "    with torch.no_grad():\n",
        "        # Pass the batch of test data through the model\n",
        "        output = model(input)\n",
        "\n",
        "    # Measure the end time of inference\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Compute the inference time for the batch\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Accumulate the inference time and update the total number of batches\n",
        "    total_time += inference_time\n",
        "    total_batches += 1\n",
        "\n",
        "# Calculate the average inference time\n",
        "avg_inference_time = total_time / total_batches\n",
        "\n",
        "# Print the average inference time in seconds\n",
        "print(\"Average Inference Time: {:.4f} seconds\".format(avg_inference_time))\n"
      ],
      "metadata": {
        "id": "XCjacWgl14ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pybL2aIR3l5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}